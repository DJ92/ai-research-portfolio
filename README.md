# AI Research Portfolio

A collection of projects demonstrating practical AI research skills, with focus on LLM capabilities, evaluation, and production-ready implementations.

## ğŸ¯ Purpose

This portfolio showcases:
- **Rigorous Evaluation**: Systematic measurement of AI system performance
- **Tool Use & Agents**: Function calling, planning, and autonomous behavior
- **Prompt Engineering**: Techniques for reliable LLM outputs
- **Production Systems**: RAG, guardrails, and monitoring
- **Research Mindset**: Reproducible experiments, failure analysis, baselines

## ğŸ“‚ Projects

### 1. LLM Evaluation Framework
**Status**: ğŸš§ In Progress

A comprehensive framework for evaluating LLM outputs across multiple dimensions:
- Accuracy, coherence, and factuality metrics
- Automated evaluation using LLM-as-judge
- Human evaluation workflows
- A/B testing infrastructure

**Tech**: Python, OpenAI/Anthropic APIs, pytest, pandas

[â†’ View Project](./01-llm-evaluation/)

---

### 2. Tool Use & Function Calling
**Status**: ğŸ“‹ Planned

Demonstrating reliable function calling and tool use patterns:
- Function calling with schema validation
- Multi-step reasoning with tools
- Error handling and retry logic
- Evaluation of tool selection accuracy

**Tech**: Python, Claude API, pydantic

[â†’ View Project](./02-tool-use/)

---

### 3. Prompt Engineering Lab
**Status**: ğŸ“‹ Planned

Systematic exploration of prompt engineering techniques:
- Chain-of-thought prompting
- Few-shot learning strategies
- Self-consistency and reasoning
- Prompt optimization methods

**Tech**: Python, Multiple LLM providers, notebooks

[â†’ View Project](./03-prompt-engineering/)

---

### 4. Production RAG System
**Status**: ğŸ“‹ Planned

End-to-end RAG implementation with evaluation:
- Document chunking strategies
- Embedding model comparison
- Retrieval quality metrics (MRR, NDCG)
- Answer quality evaluation

**Tech**: Python, ChromaDB/Pinecone, sentence-transformers

[â†’ View Project](./04-rag-system/)

---

### 5. LLM Agent Architecture
**Status**: ğŸ“‹ Planned

Building and evaluating autonomous agents:
- ReAct pattern implementation
- Planning and task decomposition
- Memory systems
- Agent evaluation frameworks

**Tech**: Python, LangChain/LlamaIndex, custom implementations

[â†’ View Project](./05-agent-architecture/)

---

## ğŸ›  Tech Stack

![Python](https://img.shields.io/badge/Python-3.10+-blue?logo=python&logoColor=white)
![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red?logo=pytorch&logoColor=white)
![OpenAI](https://img.shields.io/badge/OpenAI-API-412991?logo=openai&logoColor=white)
![Anthropic](https://img.shields.io/badge/Anthropic-Claude-black?logo=anthropic&logoColor=white)

## ğŸ“Š Evaluation Philosophy

All projects follow these principles:
1. **Quantitative Metrics**: Clear, measurable success criteria
2. **Baselines**: Compare against simple baselines and SOTA
3. **Failure Analysis**: Document and analyze failure modes
4. **Reproducibility**: All experiments have fixed seeds and configs
5. **Cost Tracking**: Monitor API costs and latency

## ğŸš€ Getting Started

Each project has its own README with:
- Problem statement and motivation
- Architecture overview
- Setup instructions
- Usage examples
- Evaluation results

## ğŸ“ Blog

I write about these projects and AI research topics on my [technical blog](https://dj92.github.io/interview-notes).

## ğŸ“« Contact

- Email: joshidheeraj1992@gmail.com
- GitHub: [@DJ92](https://github.com/DJ92)
- Blog: [dj92.github.io/interview-notes](https://dj92.github.io/interview-notes)

---

*Built with curiosity about AI capabilities and limitations. All projects prioritize measurement, reliability, and understanding over chasing benchmarks.*
